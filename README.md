AMLS_assignment20_21-SN20040326
 ======
 ## Background
Sentiment Analysis (SA) has gradually emerged in recent years for the analysis and classification of peopleâ€™s opinions, sentiments, evaluations, appraisals, attitudes and emotions towards particular entities. SA is considered to be one of the challenging research areas in the branch of natural language processing (NLP). The typical tasks in SA are usually involved detecting the sentiment attached to the sentence such as positive or negative. The object of sentiment can be a person, an even, or a topic. Social media platforms such as Twitter has Tweet text have a huge text database, so it attracts a lot of attention in the field of SA. The characteristic of Tweet text is that its language is not formal (including Internet and oral language), there may be typos, punctuation and symbols are not standardized which make decisions of tweet sentiment tricky for computers.<br>
<br>
In this project, we downloaded the Twitter dataset from [SemEval2017-Task 4](https://alt.qcri.org/semeval2017/task4/index.php?id=data-and-tools). Two tasks are included in this project:<br>
* A: Message polarity classification: For a given message, classify whether it is positive, negative or neutral sentiment.<br>
* B: Topic-based message polarity classification: The topic of each message is given and the message is positive or negative.<br>

Based on the above two tasks, we propose two text vectorization methods: bag of words (BoW) and word2vec, and three ML models: RF, RNN and LSTM
## Install
### Requirement
* Python 3.6+<br>
* macOS, Linux or Windows
### Installation Options
Go check them out if you do not have them locally installed.
```python
import numpy
import pandas
import sklearn
import matplotlib
import tensorflow
import nltk
import gensim
```
Make sure your tensorflow version is higher than 2.0 <br>
```python
print(tf.__version__)
```
## File Description
### Base Directory
* **main.py** can be excuted to run the whole project.
* **Datasets folder**: Please use your own dataset.
* The file folder A, B contain code fles for each task. Basically, they cannot run separately but need to rely on the support of main.py
>>In this file, we shall only look at and call
>>```python
>>def pre_processing(num, label, root_path, point, split = True)
>>```
* **CV2_load_data.py** is done independently by us for importing image pixel matrix sets and labels.
>>In this file, we shall only look at and call
>>```python
>>def pre_processing(num, label, root_path, split = True)
>>```
>>*num* is the number of images to be imported, *label* is the index of the labels, *root_path* is the location path of the filea called, *split* determines whether split the dataset into training set and validation set. The imported pixels will be compressed to 50*50. If you are not satisfied with this compression,
>>```python
>>def load_cv2_data(num, root_path):
>>     img_path = 'img/'
>>     X = np.zeros((num,50,50,3))
>>     for file in range(num):
>>         image = cv2.imread(root_path + img_path + '{}'.format(file) +'.png', 1)
>>         X = cv2.resize(image,(50,50))
>>    return X
>>```
>>you can modify line 3 and line 6
### Folder A
* **RF.py** contains a lot of defined function which can be called in **main.py**. 
>>Specifically, it includes hyper-parameter selection by using GridSearchCV `RF_ParameterTuning()`, model construction `RF_modeling()`, accuracy report `acc()`, and learning curve plotting `plot_learning_curve()`
### Folder B
* **B1 B1_CNN.py** is the most accurate model and will be called in **main.py**. 
>>Specifically, it includes model construction `B1_CNN()`, accuracy report `B1_acc()`, confusion matrix plotting `plot_confusion_matrix()`, loss curve plotting `plot_loss_curve()`, accuracy curve plotting `plot_accuracy_curve()`.
* **B1_MLP.py** is not called in **main.py** and can be executed separately. 
>>Specifically, it includes On hot coding transformation `On_Hot_Coding()`, model construction `allocate_weights_and_biases()+multilayer_perceptron()`, hyper-parameter setting, accuracy report<br>
>>To ensure that it can be run separately, please `import Dlib_load_data` in the correct path and copy **shape_predictor_68_face_landmarks.dat**
* **B1_RF.py** is not called in **main.py** and cannot be executed separately.
>>Specifically, it includes hyper-parameter selection by using GridSearchCV `B1_RF_ParameterTuning()`, model construction `B1_RF()`, accuracy report `B1_acc()`, and learning curve plotting `plot_learning_curve()`

## Usage
Make sure your own datasets are in the same directory with **main.py** and have the following structure (subfiles)<br>
* Datasets
> * SemEval2017-task4-dev.subtask-A.english.INPUT.txt
> * SemEval2017-task4-dev.subtask-BD.english.INPUT.txt

Remember, if datasets are placed in the wrong path or missing subfiles, the main fuction will not run.
<br>
Next, just enter the following code in the command window
```
python main.py
```
Then, you can see the accuracy socre for training set and test set in each task<br>

***Hint: It will takes few minutes to executed.*** (The testing computer is configured as CPU: i7-10750H, RAM: 16GB, GPU: Nvidia GeForce RTX 2060, and it taks about 5 min to run this code)
## Contributors
This project exists thanks to all the people who contribute.<br>
[![Jingcheng Wang](https://avatars3.githubusercontent.com/u/72794136?s=60&v=4 "Jingcheng Wang")](https://github.com/Jingcheng-WANG)
